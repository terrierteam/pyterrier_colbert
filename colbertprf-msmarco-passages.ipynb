{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eastern-smile",
   "metadata": {},
   "source": [
    "# ColBERT PRF Demo - TREC Deep Learning Track 2019 & 2020 on MSMARCO Passage\n",
    "\n",
    "\n",
    "This notebook demonstrates the application of ColBERT PRF, a pseudo-relevance feedback approach for ColBERT dense retrieval.\n",
    "\n",
    "**Citation**\n",
    "Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis. Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval. In Proceedings of ICTIR 2021. https://arxiv.org/abs/2106.11251\n",
    "\n",
    "\n",
    "**Pre-requisites**\n",
    "This requires PyTerrier, pyterrier_colbert and FAISS-GPU to be installed and working.\n",
    "\n",
    "```python\n",
    "#conda install faiss-gpu==0.6.3\n",
    "!pip install python-terrier\n",
    "!pip install git+https://github.com/cmacdonald/pyterrier_colbert.git\n",
    "```\n",
    "\n",
    "See the [pyterrier_colbert README](https://github.com/terrierteam/pyterrier_colbert/blob/main/README.md) for more information on pre-requisites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-bulgarian",
   "metadata": {},
   "source": [
    "## PyTerrier Setup\n",
    "\n",
    "Ensure that FAISS-GPU is installed and working, and setup PyTerrier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "auburn-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "assert faiss.get_num_gpus() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "macro-serial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.6.0 has loaded Terrier 5.5 (built by craigmacdonald on 2021-05-20 13:12)\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "pt.init()\n",
    "\n",
    "dataset = pt.get_dataset(\"trec-deep-learning-passages\")\n",
    "checkpoint=\"http://www.dcs.gla.ac.uk/~craigm/ecir2021-tutorial/colbert_model_checkpoint.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-tiffany",
   "metadata": {},
   "source": [
    "## PyTerrier COLBERT setup\n",
    "\n",
    "This notebook assumes that you already have an index for the MSMARCO passage ranking corpus. If not, it can be created as follows:\n",
    "```python\n",
    "from pyterrier_colbert.indexing import ColBERTIndexer\n",
    "indexer = ColBERTIndexer(checkpoint, \"/path/to/index\", \"index_name\", ids=True)\n",
    "indexer.index(dataset.get_corpus_iter())\n",
    "```\n",
    "\n",
    "We use a ColBERT checkpoint trained by the University of Glasgow on the MSMARCO passage ranking triples file for 44k batches. Its available online, and our version of ColBERT can download that automatically. Loading the ColBERT model will take a minute or so. You can ignore warnings about training on down-stream task and Torch versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "actual-louisville",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/pyterrier_colbert/pyterrier_colbert/ranking.py:243: UserWarning: Gpu disabled, YMMV\n",
      "  warn(\"Gpu disabled, YMMV\")\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ColBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 22, 14:18:53] #> Loading model checkpoint.\n",
      "[Jun 22, 14:18:53] #> Loading checkpoint http://www.dcs.gla.ac.uk/~craigm/ecir2021-tutorial/colbert_model_checkpoint.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/colbertAttempt3/lib/python3.7/site-packages/torch/hub.py:452: UserWarning: Falling back to the old format < 1.6. This support will be deprecated in favor of default zipfile format introduced in 1.6. Please redo torch.save() to save it in the new zipfile format.\n",
      "  warnings.warn('Falling back to the old format < 1.6. This support will be '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 22, 14:19:04] #> checkpoint['epoch'] = 0\n",
      "[Jun 22, 14:19:04] #> checkpoint['batch'] = 44500\n"
     ]
    }
   ],
   "source": [
    "from pyterrier_colbert.ranking import ColBERTFactory\n",
    "#update this to the location of your ColBERT index for MSMARCO passage ranking.\n",
    "index=(\"/nfs/indices/colbert_passage\",\"index_name3\")\n",
    "\n",
    "#our GPU didnt have enough memory for the FAISS index and ColBERT PRF, so we set gpu=False here\n",
    "pytcolbert = ColBERTFactory(checkpoint, *index, gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-spare",
   "metadata": {},
   "source": [
    "Next, we instantiate the standard dense retrieval pipeline for ColBERT. The first time this is instantiated, it will take some time, as the FAISS index and the embeddings index is loaded into memory (5-7 minutes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "applied-vampire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 22, 14:19:05] #> Loading the FAISS index from /nfs/indices/colbert_passage/index_name3/ivfpq.faiss ..\n",
      "[Jun 22, 14:19:40] #> Building the emb2pid mapping..\n",
      "[Jun 22, 14:20:13] len(self.emb2pid) = 687989391\n",
      "Loading reranking index, memtype=mem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading index shards to memory: 100%|██████████| 24/24 [07:11<00:00, 17.98s/shard]\n"
     ]
    }
   ],
   "source": [
    "dense_e2e = pytcolbert.end_to_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-capitol",
   "metadata": {},
   "source": [
    "Lets have a look at the output from dense_e2e for a given query - you can see that the docnos are ranked by descending score. The columns include the query tokens and their embedded representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surface-farming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "      <th>docid</th>\n",
       "      <th>query_toks</th>\n",
       "      <th>query_embs</th>\n",
       "      <th>score</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>1</td>\n",
       "      <td>chemical reactions</td>\n",
       "      <td>5453527</td>\n",
       "      <td>[tensor(101), tensor(1), tensor(5072), tensor(...</td>\n",
       "      <td>[[tensor(0.0681), tensor(-0.0083), tensor(0.11...</td>\n",
       "      <td>28.381031</td>\n",
       "      <td>5453527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>1</td>\n",
       "      <td>chemical reactions</td>\n",
       "      <td>7605154</td>\n",
       "      <td>[tensor(101), tensor(1), tensor(5072), tensor(...</td>\n",
       "      <td>[[tensor(0.0681), tensor(-0.0083), tensor(0.11...</td>\n",
       "      <td>27.986580</td>\n",
       "      <td>7605154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>1</td>\n",
       "      <td>chemical reactions</td>\n",
       "      <td>1833253</td>\n",
       "      <td>[tensor(101), tensor(1), tensor(5072), tensor(...</td>\n",
       "      <td>[[tensor(0.0681), tensor(-0.0083), tensor(0.11...</td>\n",
       "      <td>27.973652</td>\n",
       "      <td>1833253</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>1</td>\n",
       "      <td>chemical reactions</td>\n",
       "      <td>1833252</td>\n",
       "      <td>[tensor(101), tensor(1), tensor(5072), tensor(...</td>\n",
       "      <td>[[tensor(0.0681), tensor(-0.0083), tensor(0.11...</td>\n",
       "      <td>27.823465</td>\n",
       "      <td>1833252</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>1</td>\n",
       "      <td>chemical reactions</td>\n",
       "      <td>7575552</td>\n",
       "      <td>[tensor(101), tensor(1), tensor(5072), tensor(...</td>\n",
       "      <td>[[tensor(0.0681), tensor(-0.0083), tensor(0.11...</td>\n",
       "      <td>27.791912</td>\n",
       "      <td>7575552</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     qid               query    docid  \\\n",
       "1590   1  chemical reactions  5453527   \n",
       "787    1  chemical reactions  7605154   \n",
       "1786   1  chemical reactions  1833253   \n",
       "1785   1  chemical reactions  1833252   \n",
       "1692   1  chemical reactions  7575552   \n",
       "\n",
       "                                             query_toks  \\\n",
       "1590  [tensor(101), tensor(1), tensor(5072), tensor(...   \n",
       "787   [tensor(101), tensor(1), tensor(5072), tensor(...   \n",
       "1786  [tensor(101), tensor(1), tensor(5072), tensor(...   \n",
       "1785  [tensor(101), tensor(1), tensor(5072), tensor(...   \n",
       "1692  [tensor(101), tensor(1), tensor(5072), tensor(...   \n",
       "\n",
       "                                             query_embs      score    docno  \\\n",
       "1590  [[tensor(0.0681), tensor(-0.0083), tensor(0.11...  28.381031  5453527   \n",
       "787   [[tensor(0.0681), tensor(-0.0083), tensor(0.11...  27.986580  7605154   \n",
       "1786  [[tensor(0.0681), tensor(-0.0083), tensor(0.11...  27.973652  1833253   \n",
       "1785  [[tensor(0.0681), tensor(-0.0083), tensor(0.11...  27.823465  1833252   \n",
       "1692  [[tensor(0.0681), tensor(-0.0083), tensor(0.11...  27.791912  7575552   \n",
       "\n",
       "      rank  \n",
       "1590     0  \n",
       "787      1  \n",
       "1786     2  \n",
       "1785     3  \n",
       "1692     4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_e2e.search(\"chemical reactions\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-workshop",
   "metadata": {},
   "source": [
    "## ColBERT PRF\n",
    "\n",
    "Now we instantiate the ColBERT PRF pipelines - one that reranks the initial set, and one that performs a new FAISS retrieval. There is a small delay the first time these pipelines are instantiated, as the collection and document frequencies are calculated by scanning the embeddings index (~3 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accepting-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 22, 14:27:38] #> Building the emb2tid mapping..\n",
      "687989391\n",
      "Computing collection frequencies\n",
      "Done\n",
      "Loading doclens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4504/8841823 [00:00<03:16, 45028.23d/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing document frequencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8841823/8841823 [03:11<00:00, 46080.63d/s]\n",
      "100%|██████████| 30522/30522 [00:00<00:00, 292554.98it/s]\n",
      "  0%|          | 0/30522 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30522/30522 [00:00<00:00, 294430.20it/s]\n"
     ]
    }
   ],
   "source": [
    "prf_rank = pytcolbert.prf(rerank=False)\n",
    "prf_rerank = pytcolbert.prf(rerank=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-private",
   "metadata": {},
   "source": [
    "A ColBERT PRF pipeline can be searched too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "liable-illustration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "      <th>docid</th>\n",
       "      <th>query_toks</th>\n",
       "      <th>query_embs</th>\n",
       "      <th>query_weights</th>\n",
       "      <th>score</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>1</td>\n",
       "      <td>chemical reactions</td>\n",
       "      <td>7605154</td>\n",
       "      <td>[##´, vinegar, baking, reactions, substances, ...</td>\n",
       "      <td>[[tensor(0.0681), tensor(-0.0083), tensor(0.11...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "      <td>64.681244</td>\n",
       "      <td>7605154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3050</th>\n",
       "      <td>1</td>\n",
       "      <td>chemical reactions</td>\n",
       "      <td>2765749</td>\n",
       "      <td>[##´, vinegar, baking, reactions, substances, ...</td>\n",
       "      <td>[[tensor(0.0681), tensor(-0.0083), tensor(0.11...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "      <td>60.661751</td>\n",
       "      <td>2765749</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>1</td>\n",
       "      <td>chemical reactions</td>\n",
       "      <td>5453527</td>\n",
       "      <td>[##´, vinegar, baking, reactions, substances, ...</td>\n",
       "      <td>[[tensor(0.0681), tensor(-0.0083), tensor(0.11...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "      <td>59.867943</td>\n",
       "      <td>5453527</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7359</th>\n",
       "      <td>1</td>\n",
       "      <td>chemical reactions</td>\n",
       "      <td>1833253</td>\n",
       "      <td>[##´, vinegar, baking, reactions, substances, ...</td>\n",
       "      <td>[[tensor(0.0681), tensor(-0.0083), tensor(0.11...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "      <td>59.789986</td>\n",
       "      <td>1833253</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     qid               query    docid  \\\n",
       "673    1  chemical reactions  7605154   \n",
       "3050   1  chemical reactions  2765749   \n",
       "3256   1  chemical reactions  5453527   \n",
       "7359   1  chemical reactions  1833253   \n",
       "\n",
       "                                             query_toks  \\\n",
       "673   [##´, vinegar, baking, reactions, substances, ...   \n",
       "3050  [##´, vinegar, baking, reactions, substances, ...   \n",
       "3256  [##´, vinegar, baking, reactions, substances, ...   \n",
       "7359  [##´, vinegar, baking, reactions, substances, ...   \n",
       "\n",
       "                                             query_embs  \\\n",
       "673   [[tensor(0.0681), tensor(-0.0083), tensor(0.11...   \n",
       "3050  [[tensor(0.0681), tensor(-0.0083), tensor(0.11...   \n",
       "3256  [[tensor(0.0681), tensor(-0.0083), tensor(0.11...   \n",
       "7359  [[tensor(0.0681), tensor(-0.0083), tensor(0.11...   \n",
       "\n",
       "                                          query_weights      score    docno  \\\n",
       "673   [tensor(1.), tensor(1.), tensor(1.), tensor(1....  64.681244  7605154   \n",
       "3050  [tensor(1.), tensor(1.), tensor(1.), tensor(1....  60.661751  2765749   \n",
       "3256  [tensor(1.), tensor(1.), tensor(1.), tensor(1....  59.867943  5453527   \n",
       "7359  [tensor(1.), tensor(1.), tensor(1.), tensor(1....  59.789986  1833253   \n",
       "\n",
       "      rank  \n",
       "673      0  \n",
       "3050     1  \n",
       "3256     2  \n",
       "7359     3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prf_rank.search(\"chemical reactions\").head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-laser",
   "metadata": {},
   "source": [
    "In the output, you can see that the \"query_weights\" column is present, showing the weight of the expansion embeddings (1.0 in this case, the same as the original query embeddings.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-howard",
   "metadata": {},
   "source": [
    "The parameters of ColBERT PRF can be varied by using the kwargs of the `pytcolbert.prf()` method. These are as follows (extracted from the documentation):\n",
    " - `rerank`(bool): Whether to rerank the initial documents, or to perform a new set retrieve to gather new documents.\n",
    " - `fb_docs`(int): Number of passages to use as feedback. Defaults to 3. \n",
    " - `k`(int): Number of clusters to apply on the embeddings of the top K documents. Defaults to 24.\n",
    " - `fb_embs`(int): Number of expansion embeddings to add to the query. Defaults to 10.\n",
    " - `beta`(float): Weight of the new embeddings compared to the original emebddings. Defaults to 1.0.\n",
    " \n",
    "For example, a pipe with different parameter configuration can be instantiated as follows:\n",
    "```python\n",
    "prf_pipe2 = pytcolbert.prf(rerank=False, fb_docs=2, k=30, fb_embs=10, beta=0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-average",
   "metadata": {},
   "source": [
    "## Evaluation on TREC 2019\n",
    "\n",
    "We now compare standard ColBERT dense retrieval with the ranking and reranking pipelines of ColBERT PRF using the TREC 2019 Deep Learning track topics & qrels. We use the same evaluation measures as the [TREC 2019 Deep Learning track Overview paper](https://arxiv.org/abs/2003.07820).\n",
    "\n",
    "Mean response times are higher than those in the paper due to non-use of GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intimate-conclusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>AP(rel=2)@1000</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>RR(rel=2)@10</th>\n",
       "      <th>mrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ColBERT E2E</td>\n",
       "      <td>0.430957</td>\n",
       "      <td>0.693407</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>2348.040845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ColBERT-PRF Ranker beta=1</td>\n",
       "      <td>0.543034</td>\n",
       "      <td>0.735153</td>\n",
       "      <td>0.885659</td>\n",
       "      <td>7382.965954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ColBERT-PRF ReRanker beta=1</td>\n",
       "      <td>0.503836</td>\n",
       "      <td>0.736944</td>\n",
       "      <td>0.885659</td>\n",
       "      <td>5962.463029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  AP(rel=2)@1000   nDCG@10  RR(rel=2)@10  \\\n",
       "0                  ColBERT E2E        0.430957  0.693407      0.852713   \n",
       "1    ColBERT-PRF Ranker beta=1        0.543034  0.735153      0.885659   \n",
       "2  ColBERT-PRF ReRanker beta=1        0.503836  0.736944      0.885659   \n",
       "\n",
       "           mrt  \n",
       "0  2348.040845  \n",
       "1  7382.965954  \n",
       "2  5962.463029  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyterrier.measures import *\n",
    "pt.Experiment(\n",
    "    [\n",
    "        dense_e2e,\n",
    "        prf_rank,\n",
    "        prf_rerank\n",
    "    ],\n",
    "    dataset.get_topics('test-2019'),\n",
    "    dataset.get_qrels('test-2019'),\n",
    "    eval_metrics=[ AP(rel=2)@1000, nDCG@10, RR(rel=2)@10, \"mrt\"],\n",
    "    batch_size=10,\n",
    "    drop_unused=True,\n",
    "    names = [\"ColBERT E2E\",\"ColBERT-PRF Ranker beta=1\",\"ColBERT-PRF ReRanker beta=1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-hands",
   "metadata": {},
   "source": [
    "## Evaluation on TREC 2020 \n",
    "\n",
    "Further, the same experiment can be performed on the TREC 2020 topics and qrels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "turkish-equation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>AP(rel=2)@1000</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>RR(rel=2)@10</th>\n",
       "      <th>mrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ColBERT E2E</td>\n",
       "      <td>0.464902</td>\n",
       "      <td>0.687093</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>2341.704021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ColBERT-PRF Ranker beta=1</td>\n",
       "      <td>0.496238</td>\n",
       "      <td>0.699296</td>\n",
       "      <td>0.837654</td>\n",
       "      <td>7302.986394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ColBERT-PRF ReRanker beta=1</td>\n",
       "      <td>0.491899</td>\n",
       "      <td>0.700620</td>\n",
       "      <td>0.837654</td>\n",
       "      <td>6025.785316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  AP(rel=2)@1000   nDCG@10  RR(rel=2)@10  \\\n",
       "0                  ColBERT E2E        0.464902  0.687093      0.850000   \n",
       "1    ColBERT-PRF Ranker beta=1        0.496238  0.699296      0.837654   \n",
       "2  ColBERT-PRF ReRanker beta=1        0.491899  0.700620      0.837654   \n",
       "\n",
       "           mrt  \n",
       "0  2341.704021  \n",
       "1  7302.986394  \n",
       "2  6025.785316  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [\n",
    "        dense_e2e,\n",
    "        prf_rank,\n",
    "        prf_rerank\n",
    "    ],\n",
    "    dataset.get_topics('test-2020'),\n",
    "    dataset.get_qrels('test-2020'),\n",
    "    eval_metrics=[ AP(rel=2)@1000, nDCG@10, RR(rel=2)@10, \"mrt\"],\n",
    "    drop_unused=True,\n",
    "    batch_size=10,\n",
    "    names = [\"ColBERT E2E\",\"ColBERT-PRF Ranker beta=1\",\"ColBERT-PRF ReRanker beta=1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-reducing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colbertAttempt3",
   "language": "python",
   "name": "colbertattempt3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
